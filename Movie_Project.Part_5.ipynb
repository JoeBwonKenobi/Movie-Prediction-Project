{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14e295c",
   "metadata": {},
   "source": [
    "# **Movie Project Part 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f8cb8",
   "metadata": {},
   "source": [
    "Joe Lardie \n",
    "\n",
    "March 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d6a30",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e83acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576386e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f286c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c767577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import gzip\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Numerical and Data Analysis Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "# Database Management\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import create_database, database_exists\n",
    "from sqlalchemy.types import *\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Additional Libraries\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import tmdbsimple as tmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef81a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akas.csv.gz',\n",
       " 'basics.csv.gz',\n",
       " 'final_tmdb_data_2000 (1).csv.gz',\n",
       " 'final_tmdb_data_2000 (2).csv.gz',\n",
       " 'final_tmdb_data_2000 (3).csv.gz',\n",
       " 'final_tmdb_data_2000.csv.gz',\n",
       " 'final_tmdb_data_2001.csv.gz',\n",
       " 'final_tmdb_data_2002.csv.gz',\n",
       " 'final_tmdb_data_2003.csv.gz',\n",
       " 'final_tmdb_data_2004.csv.gz',\n",
       " 'final_tmdb_data_2005.csv.gz',\n",
       " 'final_tmdb_data_2006.csv.gz',\n",
       " 'final_tmdb_data_2007.csv.gz',\n",
       " 'final_tmdb_data_2008.csv.gz',\n",
       " 'final_tmdb_data_2009.csv.gz',\n",
       " 'final_tmdb_data_2010.csv.gz',\n",
       " 'final_tmdb_data_2011.csv.gz',\n",
       " 'final_tmdb_data_2012.csv.gz',\n",
       " 'final_tmdb_data_2013.csv.gz',\n",
       " 'final_tmdb_data_2014.csv.gz',\n",
       " 'final_tmdb_data_2015.csv.gz',\n",
       " 'final_tmdb_data_2016.csv.gz',\n",
       " 'final_tmdb_data_2017.csv.gz',\n",
       " 'final_tmdb_data_2018.csv.gz',\n",
       " 'final_tmdb_data_2019.csv.gz',\n",
       " 'final_tmdb_data_2020.csv.gz',\n",
       " 'ratings.csv.gz',\n",
       " 'tmdb_api_results_2000.csv.gz',\n",
       " 'tmdb_api_results_2000.json',\n",
       " 'tmdb_api_results_2001.json',\n",
       " 'tmdb_api_results_2002.json',\n",
       " 'tmdb_api_results_2003.json',\n",
       " 'tmdb_api_results_2004.json',\n",
       " 'tmdb_api_results_2005.json',\n",
       " 'tmdb_api_results_2006.json',\n",
       " 'tmdb_api_results_2007.json',\n",
       " 'tmdb_api_results_2008.json',\n",
       " 'tmdb_api_results_2009.json',\n",
       " 'tmdb_api_results_2010.json',\n",
       " 'tmdb_api_results_2011.json',\n",
       " 'tmdb_api_results_2012.json',\n",
       " 'tmdb_api_results_2013.json',\n",
       " 'tmdb_api_results_2014.json',\n",
       " 'tmdb_api_results_2015.json',\n",
       " 'tmdb_api_results_2016.json',\n",
       " 'tmdb_api_results_2017.json',\n",
       " 'tmdb_api_results_2018.json',\n",
       " 'tmdb_api_results_2019.json',\n",
       " 'tmdb_api_results_2020.json',\n",
       " 'tmdb_api_results_2021.json',\n",
       " 'tmdb_api_results_2022.json',\n",
       " 'tmdb_results_combined.csv.gz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLDER = \"Data/\"\n",
    "os.makedirs(FOLDER, exist_ok=True)\n",
    "os.listdir(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5619bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataframe from project part 1 as basics:\n",
    "basics = pd.read_csv('Data/basics.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4461ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Ratings data\n",
    "ratings = pd.read_csv('Data/ratings.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b0d4f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import akas data\n",
    "akas = pd.read_csv('Data/akas.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74fb85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basics\n",
    "tmdb = pd.read_csv('Data/tmdb_results_combined.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3bd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sql(df,primary=None):\n",
    "    sql_schema = {key: None for key in df.columns}\n",
    "    #Create schema to convert col.dtype to sql-types\n",
    "    for col in df.columns:\n",
    "       # print (f\"{col} is type:{basics[col].dtype}\")\n",
    "        if df[col].dtype == \"int64\":\n",
    "            sql_schema[col]=Integer()\n",
    "        elif df[col].dtype == \"float64\":\n",
    "            sql_schema[col]=Float()\n",
    "        elif df[col].dtype == \"object\":\n",
    "            sql_schema[col]=Text(df[col].fillna('').map(len).max()+1)\n",
    "    if primary != None:\n",
    "        #Change the primary key to type String(length=...)\n",
    "        sql_schema[primary] = String(df[primary].fillna('').map(len).max()+1)\n",
    "    return sql_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe128d",
   "metadata": {},
   "source": [
    "## **Creating MySQL DataBase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8491a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create connection string using credintials\n",
    "# connection = \"dialect+driver://username:password@host:port/database\"\n",
    "connection_str = \"mysql+pymysql://root:Root@localhost/movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bffc041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the engine:\n",
    "engine = create_engine(connection_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b06c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database already exists\n"
     ]
    }
   ],
   "source": [
    "# Check if the database exists. If not, create it.\n",
    "if database_exists(connection_str) == False:\n",
    "  create_database(connection_str)\n",
    "else:\n",
    "  print('The database already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "209457d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for database existance:\n",
    "database_exists(connection_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac4d12c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tconst             object\n",
       "titleType          object\n",
       "primaryTitle       object\n",
       "originalTitle      object\n",
       "isAdult             int64\n",
       "startYear           int64\n",
       "endYear           float64\n",
       "runtimeMinutes    float64\n",
       "genres             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the dtypes of your dataframe: (df.dtypes).\n",
    "basics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c9399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tconst': String(length=11),\n",
       " 'titleType': Text(length=6),\n",
       " 'primaryTitle': Text(length=243),\n",
       " 'originalTitle': Text(length=243),\n",
       " 'isAdult': Integer(),\n",
       " 'startYear': Integer(),\n",
       " 'endYear': Float(),\n",
       " 'runtimeMinutes': Float(),\n",
       " 'genres': Text(length=30)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use custom function to convert to sql-ready\n",
    "basics_schema = df_to_sql(basics,\"tconst\")\n",
    "basics_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e61cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tconst (VARCHAR(11)) not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save to sql with dtype and index=False\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mbasics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle_basics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasics_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\generic.py:3087\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[0;32m   2891\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   3085\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[1;32m-> 3087\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\sql.py:842\u001b[0m, in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    838\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    839\u001b[0m     )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mto_sql(\n\u001b[0;32m    843\u001b[0m         frame,\n\u001b[0;32m    844\u001b[0m         name,\n\u001b[0;32m    845\u001b[0m         if_exists\u001b[38;5;241m=\u001b[39mif_exists,\n\u001b[0;32m    846\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    847\u001b[0m         index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m    848\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    849\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    850\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    851\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    852\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    853\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs,\n\u001b[0;32m    854\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\sql.py:2839\u001b[0m, in \u001b[0;36mSQLiteDatabase.to_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m   2837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col, my_type \u001b[38;5;129;01min\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2838\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(my_type, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 2839\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2841\u001b[0m table \u001b[38;5;241m=\u001b[39m SQLiteTable(\n\u001b[0;32m   2842\u001b[0m     name,\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2848\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   2849\u001b[0m )\n\u001b[0;32m   2850\u001b[0m table\u001b[38;5;241m.\u001b[39mcreate()\n",
      "\u001b[1;31mValueError\u001b[0m: tconst (VARCHAR(11)) not a string"
     ]
    }
   ],
   "source": [
    "# Save to sql with dtype and index=False\n",
    "basics.to_sql('title_basics',engine,dtype=basics_schema,if_exists='replace',\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065414b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the query to ADD PRIMARY KEY\n",
    "engine.execute('ALTER TABLE title_basics ADD PRIMARY KEY (`tconst`);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56221487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query the table and show first 5 rows\n",
    "q = '''\n",
    "SELECT *\n",
    "FROM title_basics\n",
    "Limit 5;\n",
    "'''\n",
    "pd.read_sql_query(q, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd559582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings colummns\n",
    "ratings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bec04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of Ratings data\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2142d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique ratings count\n",
    "ratings['tconst'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103705cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a ratings_id map by pairing the unique ratings with an incrementing integer\n",
    "ratings_id = range(len(ratings['tconst'].unique()))\n",
    "ratings_map = dict(zip(ratings['tconst'].unique(), ratings_id))\n",
    "#Add ratings_id primary key column\n",
    "ratings[\"id\"] = ratings[\"tconst\"].map(ratings_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_schema = df_to_sql(ratings)\n",
    "ratings_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889aadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sql with dtype and index=False\n",
    "ratings.to_sql('title_ratings',engine,dtype=ratings_schema,if_exists='replace',\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the query to ADD PRIMARY KEY\n",
    "engine.execute('ALTER TABLE title_ratings ADD PRIMARY KEY (`id`);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ea099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query the table and show first 5 rows\n",
    "q = '''\n",
    "SELECT *\n",
    "FROM title_ratings\n",
    "Limit 5;\n",
    "'''\n",
    "pd.read_sql_query(q, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee493ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a col with a list of genres\n",
    "basics['genres_split'] = basics['genres'].str.split(',')\n",
    "basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31726a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_genres = basics.explode('genres_split')\n",
    "exploded_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique genres\n",
    "unique_genres = sorted(exploded_genres['genres_split'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save just tconst and genres_split as new df\n",
    "title_genres = exploded_genres[['tconst', 'genres_split']].copy()\n",
    "title_genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634cb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the genre mapper dictionary\n",
    "genre_ints = range(len(unique_genres))\n",
    "genre_map = dict(zip(unique_genres, genre_ints))\n",
    "genre_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a dictionary with list of unique genres as the key and the new iteger id as values\n",
    "genre_id_map = dict(zip(unique_genres, range(len(unique_genres))))\n",
    "genre_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basics['genres_split'] = basics['genres_split'].apply(lambda x: tuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed86e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make new integer genre_id and drop string genres\n",
    "basics['genre_id'] = basics['genres_split'].map(genre_map)\n",
    "basics = basics.drop(columns='genres_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdb49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manaully make dataframe with named cols from the .keyd and .values\n",
    "genre_lookup = pd.DataFrame ({'Genre_Name': genre_id_map.keys(),\n",
    "                             'genre_ID':genre_id_map.values()})\n",
    "genre_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "basics['int_index'] = range(len(basics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get max string length\n",
    "max_str_len = basics['genres'].fillna('').map(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate max string lengths for object columns\n",
    "key_len = basics['tconst'].fillna('').map(len).max()\n",
    "title_len = basics['primaryTitle'].fillna('').map(len).max()\n",
    "## Create a schema dictonary using Sqlalchemy datatype objects\n",
    "df_schema = {\n",
    "    \"tconst\": String(key_len+1), \n",
    "    \"primaryTitle\": Text(title_len+1),\n",
    "    'startYear':Float(),\n",
    "    'endYear':Float(),\n",
    "    'runtimeMinutes':Integer()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6258bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sql with dtype and index=False\n",
    "basics.to_sql('title_basics',engine,dtype=df_schema,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dtypes of your dataframe: (df.dtypes).\n",
    "title_genres.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use custom function to convert to sql-ready\n",
    "title_genres_schema = df_to_sql(title_genres)\n",
    "title_genres_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sql with dtype and index=False\n",
    "title_genres.to_sql('title_genres',engine,dtype=title_genres_schema,if_exists='replace',\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97336ec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Query the table and show first 5 rows\n",
    "q = '''\n",
    "SELECT *\n",
    "FROM title_genres\n",
    "Limit 5;\n",
    "'''\n",
    "pd.read_sql_query(q, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = pd.DataFrame(basics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genres Columns\n",
    "genres.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e720f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use custom function to convert to sql-ready\n",
    "genres_schema = df_to_sql(genres)\n",
    "genres_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ef699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sql with dtype and index=False\n",
    "genres.to_sql('genres',engine,dtype=genres_schema,if_exists='replace',\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b592bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query the table and show first 5 rows\n",
    "q = '''\n",
    "SELECT *\n",
    "FROM genres\n",
    "Limit 5;\n",
    "'''\n",
    "pd.read_sql_query(q, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c62907",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d867ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You only need to keep the imdb_id, revenue, budget, and certification columns\n",
    "tmdb_req = tmdb[[\"imdb_id\",\"revenue\",\"budget\",\"certification\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use custom function to convert to sql-ready\n",
    "tmdb_schema = df_to_sql(tmdb_req,\"imdb_id\")\n",
    "tmdb_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7928c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sql with dtype and index=False\n",
    "tmdb_req.to_sql('tmdb_data',engine,dtype=tmdb_schema,if_exists='replace',\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47785f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Query the table and show first 5 rows\n",
    "q = '''\n",
    "SELECT *\n",
    "FROM tmdb_data\n",
    "Limit 5;\n",
    "'''\n",
    "pd.read_sql_query(q, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40bd1a",
   "metadata": {},
   "source": [
    "## **Part 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01578c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recursive query - extra /**/ added to string\n",
    "q = \"Data/tmdb_results_combined.csv.gz\" \n",
    "tmdb_results_combined = sorted(glob.glob(q)) \n",
    "# Showing the first 5 \n",
    "tmdb_results_combined[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc05785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Loading all files as df and appending to a list\n",
    "df_list = []\n",
    "for file in tmdb_results_combined:\n",
    "    tmdb_df = pd.read_csv(file, index_col=0)\n",
    "    df_list.append(tmdb_df)\n",
    "    \n",
    "## Concatenating the list of dfs into 1 combined\n",
    "df_combined = pd.concat(df_list)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading and Concatenating the list of dfs with 1 line\n",
    "df_combined = pd.concat([pd.read_csv(file, index_col=0) for file in tmdb_results_combined])\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277916b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to be continuous\n",
    "movie_df = df_combined.copy().reset_index()\n",
    "movie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c84953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only need imdb_id, budget, revenue, and certification\n",
    "movie_df = movie_df[['imdb_id', 'budget', 'revenue', 'certification']]\n",
    "movie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9daf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daed990",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbf032",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(movie_df.info(), movie_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8778209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename imdb_id as 'tconst' to match basics\n",
    "movie_df.rename(columns = {'imdb_id': 'tconst'}, inplace = True)\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lod in title basics\n",
    "basics = pd.read_csv('Data/basics.csv.gz')\n",
    "basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58448d6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop a couple unnecessary columns\n",
    "basics = basics.drop(columns = ['isAdult', 'titleType'])\n",
    "basics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(movie_df, basics, on = 'tconst', how = 'right')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f86e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"originalTitle\"\n",
    "df = df.drop(columns = 'originalTitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a938b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to load in and add or title ratings\n",
    "ratings = pd.read_csv('Data/ratings.csv.gz')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merg this with our df\n",
    "df = pd.merge(df, ratings, on = 'tconst', how = 'right')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the 'primaryTitle'\n",
    "df = df.dropna(subset = 'primaryTitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd898e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f707d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to assess Certification\n",
    "df['certification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Certifications that are not of interest\n",
    "\n",
    "df['certification'] = df['certification'].replace({'-':'drop', 'NC-17':'drop',\n",
    "                                                   'NR':'drop', '10': 'drop',\n",
    "                                                   'Unrated':'drop'})\n",
    "# Filter out rows with 'drop' in the certification column\n",
    "df = df[df.certification != 'drop']\n",
    "df['certification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255999aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['certification'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the single R anf PG-13 values\n",
    "df['certification'] = df['certification'].replace({'R ':'R', 'PG-13 ': 'PG-13'})\n",
    "df['certification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da16450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded5266",
   "metadata": {},
   "source": [
    "## **Hypothesis Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddeff43",
   "metadata": {},
   "source": [
    "### **Stakeholder's First Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c9a59",
   "metadata": {},
   "source": [
    "- The stakeholder's 1st question is: does the MPAA rating of a movie (G/PG/PG-13/R) affect how much revenue the movie generates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f526a1",
   "metadata": {},
   "source": [
    "Null Hypothesis:There is no difference in revenue generation between different movie ratings\n",
    "\n",
    "Alternate Hypothesis:There will be a statistical difference between revenue generation between movie certification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b6eb1c",
   "metadata": {},
   "source": [
    "Alpha = 0.05\n",
    "We will be using an ANOVA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eea71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy df\n",
    "anova_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21852ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop 0.0 revenue values\n",
    "anova_df = anova_df[anova_df.revenue != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply to show our certifications and revenue BEFORE removing outliers\n",
    "sns.barplot(data = anova_df, x = 'certification', y = 'revenue');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51035b74",
   "metadata": {},
   "source": [
    "We need to assess for outliers using Tukey's rule for Outliers\n",
    "Tukey's rule states that outliers are those values more than 1.5 times the IQR (Interquartile Range).\n",
    "This means values either:\n",
    "1.5 times lower than Q1 (Q1 - 1.5IQR)\n",
    "1.5 times higher than Q3 (Q3 + 1.5IQR)\n",
    "Therefore, we need to calculate our Q1, Q3, and IQR. Then, we can find our upper and lower limits, and drop outliers outside that range.\n",
    "\n",
    "The below code was adapted from: https://www.youtube.com/watch?v=A3gClkblXK8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1 and Q2\n",
    "Q1 = anova_df.revenue.quantile(0.25)\n",
    "Q3 = anova_df.revenue.quantile(0.75)\n",
    "Q1, Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR\n",
    "IQR = Q3 - Q1\n",
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set upper and lower limit\n",
    "lower_limit = Q1 - (1.5 * IQR)\n",
    "upper_limit = Q3 + (1.5 * IQR)\n",
    "lower_limit, upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ecb477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will find and list our outliers\n",
    "anova_df[(anova_df.revenue < lower_limit) | (anova_df.revenue > upper_limit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94866aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_no_outliers = anova_df[(anova_df.revenue > lower_limit) & (anova_df.revenue < upper_limit)]\n",
    "anova_no_outliers\n",
    "    # We can see below (from our row count) tha we have eliminated our outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9656de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to separate and analyze our groups individually. We have 4 ratings\n",
    "certifications = {}\n",
    "\n",
    "# Loop through all unique characteristics\n",
    "for i in anova_no_outliers['certification'].unique():\n",
    "    data = anova_no_outliers.loc[anova_no_outliers['certification'] == i,\n",
    "                                 'revenue'].copy()\n",
    "    \n",
    "    # Save results in dictionary\n",
    "    certifications[i] = data\n",
    "    \n",
    "certifications.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff77927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can assess normality of our certifications (groups)\n",
    "norm_results = {}\n",
    "\n",
    "for i, data in certifications.items():\n",
    "    stat, p = stats.normaltest(data)\n",
    "    \n",
    "    # Append norm_results with p-values, test stats, and size of region group\n",
    "    norm_results[i] = {'n': len(data), 'p': p, 'test stat': stat,}\n",
    "    \n",
    "# Convert to a DF\n",
    "norm_results_df = pd.DataFrame(norm_results).T\n",
    "norm_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f65c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sign (statistical significance) in pandas (more ledgible)\n",
    "norm_results_df['sig'] = norm_results_df['p'] < 0.05\n",
    "norm_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b6d95",
   "metadata": {},
   "source": [
    "\n",
    "We confirm that all regions do NOT have normal distribution.\n",
    "We have large enough groups to ignore the assumption of normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing groups for equal varience (levene test)\n",
    "stats.levene(*certifications.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfe9b3",
   "metadata": {},
   "source": [
    "We did not meet the assumption of equal variance, but we can still perform a one-way ANOVA test using the Kruskal-Wallis test (Nonparametric test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_anova = stats.kruskal(*certifications.values())\n",
    "results_anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a262938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick visualization of our data WITHOUT outliers\n",
    "sns.barplot(data = anova_no_outliers, x = 'certification', y = 'revenue');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13152afc",
   "metadata": {},
   "source": [
    "To take this one step further, this is likely do to larger crowd appeal due to lack of age restrictions of lower certifications, meaning more people view these movies. Also parents must buy tickets for both their children, and themselves when going to see a G movie (potentially, more overall tickets sold)\n",
    "Our result is < (less than) our Alpha of 0.05, which means we:\n",
    "REJECT the Null Hypothesis (There is no difference in revenue generation between different movie ratings)\n",
    "SUPPORT the Alternate Hypothesis (There will be a statistical difference between revenue generation between movie certification)\n",
    "The stakeholder's SECOND question is: Do movies with a runtime of 2 hours or more have higher budgets?\n",
    "Null Hypothesis:\n",
    "There is no difference in budget amounts for movies of 2 hours or more than movies shorter than 2 hours\n",
    "Alternate Hypothesis:\n",
    "There will be a statistical difference between budget amounts for movies of 2 hours or more than movies shorter than 2 hours\n",
    "Alpha = 0.05\n",
    "We will be using an 2 sample T-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264484de",
   "metadata": {},
   "source": [
    "### **Stakeholder's Second Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296101d",
   "metadata": {},
   "source": [
    "- The stakeholder's second question is: Do movies with a runtime of 2 hours or more have higher budgets?\n",
    "\n",
    "Null Hypothesis: There is no difference in budget amounts for movies of 2 hours or more than movies shorter than 2 hours\n",
    "\n",
    "Alternate Hypothesis: There will be a statistical difference between budget amounts for movies of 2 hours or more than movies shorter than 2 hours \n",
    "\n",
    "Alpha = 0.05 We will be using an 2 sample T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c234ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DF with no budget values = 0\n",
    "length_df = df.copy()\n",
    "length_df = length_df[length_df.budget != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 dfs for long and short movies\n",
    "long_movie = length_df.loc[df['runtimeMinutes'] >= 120.0].copy()\n",
    "short_movie = length_df.loc[df['runtimeMinutes'] < 120.0].copy()\n",
    "\n",
    "# Get new movie DF info\n",
    "display(long_movie.info(), short_movie.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a num boolean column to help us with visualizations\n",
    "length_df['short_movie'] = length_df[['runtimeMinutes']].sum(axis = 1) < 120.0 \n",
    "length_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5013635",
   "metadata": {},
   "source": [
    "Visualizing average budget differences between short (<120min) movies and long (>=120min) movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize before dealing with outliers\n",
    "fig, ax = plt.subplots(figsize = (4.2, 4.2))\n",
    "sns.barplot(data = length_df, x = 'short_movie', y = 'budget', \n",
    "            hue = 'short_movie')\n",
    "plt.title('Average buget by movie length');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values so they don't interfere with our statistical test\n",
    "long_movie.dropna(inplace = True)\n",
    "short_movie.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(long_movie.info(), short_movie.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features of interest\n",
    "long_budget = long_movie['budget']\n",
    "short_budget = short_movie['budget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb99a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers separately in both groups\n",
    "zscores_long = stats.zscore(long_budget)\n",
    "outliers = abs(zscores_long) > 3\n",
    "np.sum(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17377c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers separately in both groups\n",
    "zscores_short = stats.zscore(short_budget)\n",
    "outliers = abs(zscores_short) > 3\n",
    "np.sum(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c65d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "long_budget = long_budget[(np.abs(stats.zscore(long_budget)) < 3)]\n",
    "short_budget = short_budget[(np.abs(stats.zscore(short_budget)) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normality\n",
    "result_long_movie = stats.normaltest(df.iloc[:, 5])\n",
    "print(result_long_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normalitye\n",
    "result_short_movie = stats.normaltest(df.iloc[:, 5])\n",
    "result_short_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f4cc9",
   "metadata": {},
   "source": [
    "Both normality tests resulted in a p-value far below 0.05, indicting they are NOT normally distributed\n",
    "We will continue with our tests, however, because the group sizes are larger than 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec542df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Levene test to check for equal variance\n",
    "result_levene = stats.levene(long_budget, short_budget)\n",
    "result_levene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae51c0d",
   "metadata": {},
   "source": [
    "Our groups do NOT have equal variance\n",
    "Therefore we will include \"equal_var = False\" in our T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent t-test with equal var set to False\n",
    "result_ttest = stats.ttest_ind(long_budget, short_budget,\n",
    "                               equal_var = False)\n",
    "result_ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdfad3",
   "metadata": {},
   "source": [
    "Our result is < (less than) our Alpha of 0.05, which means we:\n",
    "REJECT the Null Hypothesis (There is no difference in budget amounts for movies of 2 hours or more than movies shorter than 2 hours)\n",
    "SUPPORT the Alternate Hypothesis (There will be a statistical difference between budget amounts for movies of 2 hours or more than movies shorter than 2 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331949e1",
   "metadata": {},
   "source": [
    "### **Stakeholder's Third Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e156e",
   "metadata": {},
   "source": [
    "- The stakeholder's THIRD question is: Does the certification (G, PG, PG-13, R) of a movie affect the movie's average rating?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13056335",
   "metadata": {},
   "source": [
    "Null Hypothesis:There is no difference in ratings between movie certifications\n",
    "\n",
    "Alternate Hypothesis:There will be a statistical difference in ratings between movie certifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5e9e7",
   "metadata": {},
   "source": [
    "Alpha = 0.05\n",
    "We will be using an ANOVA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a20fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will follow similar steps to those used in question ONE\n",
    "# Make a copy df\n",
    "anova_df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38049ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check values in averageRating column\n",
    "anova_df2['averageRating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values from certifications\n",
    "anova_df2 = anova_df2.dropna(subset = 'certification')\n",
    "anova_df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e92aa",
   "metadata": {},
   "source": [
    "There are no columns with a 0.0 value to consider (seems like they're all filled)\n",
    "Also, we can see that the ratings range from 1 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9985323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no columns with a 0.0 value to consider (seems like they're all filled)\n",
    "# Quick plot with ouliers\n",
    "sns.barplot(data = anova_df2, x = 'certification', y = 'averageRating');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Using Tukey's rule, set quartile ranges and eliminate outliers\n",
    "# Calculate Q1 and Q2\n",
    "Q1 = anova_df2.averageRating.quantile(0.25)\n",
    "Q3 = anova_df2.averageRating.quantile(0.75)\n",
    "Q1, Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR\n",
    "IQR = Q3 - Q1\n",
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f9ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set upper and lower limit\n",
    "lower_limit = Q1 - (1.5 * IQR)\n",
    "upper_limit = Q3 + (1.5 * IQR)\n",
    "lower_limit, upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf53dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and list our outliers\n",
    "anova_df2[(anova_df2.averageRating < lower_limit) | (anova_df2.averageRating > upper_limit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09693ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_no_outliers2 = anova_df2[(anova_df2.averageRating > lower_limit) & (anova_df2.averageRating < upper_limit)]\n",
    "anova_no_outliers2\n",
    "    # We can see below (from our row count) tha we have eliminated our outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bde513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to separate and analyze our groups individually. We have 4 certifications\n",
    "certifications2 = {}\n",
    "\n",
    "# Loop through all unique characteristics\n",
    "for i in anova_no_outliers2['certification'].unique():\n",
    "    data = anova_no_outliers2.loc[anova_no_outliers2['certification'] == i,\n",
    "                                 'averageRating'].copy()\n",
    "    \n",
    "    # Save results in dictionary\n",
    "    certifications2[i] = data\n",
    "    \n",
    "certifications2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can assess normality of our certifications (groups)\n",
    "norm_results2 = {}\n",
    "\n",
    "for i, data in certifications2.items():\n",
    "    stat, p = stats.normaltest(data)\n",
    "    \n",
    "    # Append norm_results with p-values, test stats, and size of region group\n",
    "    norm_results2[i] = {'n': len(data), 'p': p, 'test stat': stat,}\n",
    "    \n",
    "# Convert to a DF\n",
    "norm_results_df2 = pd.DataFrame(norm_results2).T\n",
    "norm_results_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sign (statistical significance) in pandas (more ledgible)\n",
    "norm_results_df2['sig'] = norm_results_df2['p'] < 0.05\n",
    "norm_results_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf8bf7",
   "metadata": {},
   "source": [
    "We confirm that 3 regions do NOT have normal distribution.\n",
    "We have large enough groups to ignore the assumption of normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293eecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing groups for equal varience (levene test)\n",
    "stats.levene(*certifications2.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc03b5",
   "metadata": {},
   "source": [
    "Our p-value is > 0.05, so we met our assumption of equal variance.\n",
    "We will perform a One-Way ANOVA test in this situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eacf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = stats.f_oneway(*certifications2.values())\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick visualization of our data WITHOUT outliers\n",
    "sns.barplot(data = anova_no_outliers2, x = 'certification', y = 'averageRating');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b15cf",
   "metadata": {},
   "source": [
    "Our result is < (less than) our Alpha of 0.05, which means we:\n",
    "REJECT the Null Hypothesis (There is no difference in ratings between movie certifications )\n",
    "SUPPORT the Alternate Hypothesis (There will be a statistical difference in ratings between movie certifications )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6b863",
   "metadata": {},
   "source": [
    "## **Part 5 Linear Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = anova_no_outliers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f5f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('endYear', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1aff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,y_vars='revenue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad20d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validation Split\n",
    "y = df['revenue'].copy()\n",
    "X = df.drop(columns=['revenue', 'averageRating']).copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d182c",
   "metadata": {},
   "source": [
    "## **General Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13483f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat selector\n",
    "cat_select = make_column_selector(dtype_include='object')\n",
    "cat_cols = cat_select(X_train)\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num selector\n",
    "num_select = make_column_selector(dtype_include='number')\n",
    "num_cols = num_select(X_train)\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24036814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines\n",
    "cat_pipe = make_pipeline(SimpleImputer(strategy='constant',\n",
    "                                       fill_value='MISSING'),\n",
    "                         OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy='mean'),#StandardScaler()\n",
    "                        )\n",
    "preprocessor = make_column_transformer((cat_pipe,cat_cols),\n",
    "                                        (num_pipe, num_cols), remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73519cb6",
   "metadata": {},
   "source": [
    "## **Preprocessing For Statsmodels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the tansformer\n",
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2370a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.round(preprocessor.transform(X_train), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78286c",
   "metadata": {},
   "source": [
    "## **Getting Feature Names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list \n",
    "final_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db798430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the categorical pipeline in our col transformer\n",
    "preprocessor.named_transformers_['pipeline-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1582e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using named steps dictionary to find the encoder part 1\n",
    "preprocessor.named_transformers_['pipeline-1'].named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Using named steps dictionary to find the encoder - Part 2\n",
    "ohe_step = preprocessor.named_transformers_['pipeline-1'].named_steps['onehotencoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## B) Using list-slicing to find the encoder \n",
    "ohe_step = preprocessor.named_transformers_['pipeline-1'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7ce3a",
   "metadata": {},
   "source": [
    "## **Get Feature Names out**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8a9c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now, get OHE feature names\n",
    "cat_features = ohe_step.get_feature_names_out(cat_cols)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.extend(cat_features)\n",
    "final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1e345",
   "metadata": {},
   "source": [
    "## **Numerical Features Names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.extend(num_cols)\n",
    "final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e3ea4",
   "metadata": {},
   "source": [
    "## **Transforming X_train and X_test and Remaking Data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming X_train\n",
    "X_train_df = pd.DataFrame(preprocessor.transform(X_train), columns=final_features, index=X_train.index)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad907679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming X_test\n",
    "X_test_df = pd.DataFrame(preprocessor.transform(X_test), columns=final_features, index=X_test.index)\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50cc2f6",
   "metadata": {},
   "source": [
    "## **Overwrite X dataframe to include the constant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b78288d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_df = sm.add_constant(X_train_df,has_constant='add', prepend=False)\n",
    "X_test_df = sm.add_constant(X_test_df,has_constant='add', prepend=False)\n",
    "display(X_train_df.head(2), X_test_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb65925",
   "metadata": {},
   "source": [
    "# **Statsmodels vs Sklearn(Linear Regression)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ff27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "model.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictions\n",
    "train_preds = model.predict(X_train_df)\n",
    "test_preds = model.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find r-square\n",
    "print('Training r2:', r2_score(y_train, train_preds))\n",
    "print('Testing r2:', r2_score(y_test, test_preds))\n",
    "# find mse\n",
    "print('Training MSE:', mean_squared_error(y_train, train_preds))\n",
    "print('Testing MSE:', mean_squared_error(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735aad1d",
   "metadata": {},
   "source": [
    "## **OLS Statmodels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make & fit a statmsodels OLS\n",
    "model = sm.OLS(y_train,X_train_df, hasconst=True)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit an OLS model\n",
    "model = sm.OLS(y_train,X_train_df)\n",
    "result = model.fit()\n",
    "## Use the result (not the model) to .predict\n",
    "test_preds = result.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r2 = r2_score(y_test, test_preds)\n",
    "test_mse = mean_squared_error(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The testing r-square value is {test_r2} and the testing mean squared error is {test_mse}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0285f2",
   "metadata": {},
   "source": [
    "# **Assumptions of Linearity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad794cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving list of numeric features to slice for pairplot\n",
    "num_selector = make_column_selector(dtype_include='number')\n",
    "cols = num_selector(df.drop(columns='revenue'))\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd10309",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, y_vars='revenue',x_vars=cols[:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a pairplot with regression lines\n",
    "sns.pairplot(df, y_vars='revenue',kind='reg',x_vars=cols[:5],\n",
    "             plot_kws=dict(line_kws={'color':'red', 'ls':'--'},\n",
    "                           scatter_kws={'edgecolor':'white','lw':1}));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Heat Map\n",
    "corr = df.drop(columns='revenue').corr().abs()\n",
    "sns.heatmap(corr,square=True, cmap='Greens', annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the mask to hide the upper-right of the triangle\n",
    "corr = df.drop(columns='revenue').corr().abs()\n",
    "mask = np.triu(np.ones_like(corr))\n",
    "sns.heatmap(corr,square=True, cmap='Greens', annot=True, mask=mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54db58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding revenue back to the correlation heatmap\n",
    "corr = df.corr().abs()\n",
    "mask = np.triu(np.ones_like(corr))\n",
    "sns.heatmap(corr,square=True, cmap='Greens', annot=True, mask=mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the column showing high corelation\n",
    "df = df.drop(columns=['numVotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final check for multicollinearity via correlation\n",
    "corr = df.drop(columns='revenue').corr().abs()\n",
    "mask = np.triu(np.ones_like(corr))\n",
    "sns.heatmap(corr,square=True, cmap='Greens', annot=True, mask=mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f729e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make x and y variables\n",
    "y = df['revenue'].copy()\n",
    "X = df.drop(columns=['revenue']).copy()\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, random_state=321)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make cat selector and using it to save list of column names\n",
    "cat_select = make_column_selector(dtype_include='object')\n",
    "cat_cols = cat_select(X_train)\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make num selector and using it to save list of column names\n",
    "num_select = make_column_selector(dtype_include='number')\n",
    "num_cols = num_select(X_train)\n",
    "## make pipelines\n",
    "cat_pipe = make_pipeline(SimpleImputer(strategy='constant',\n",
    "                                       fill_value='MISSING'),\n",
    "                         OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "                        )\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy='mean'))\n",
    "preprocessor = make_column_transformer( (num_pipe, num_cols),\n",
    "                                       (cat_pipe,cat_cols),\n",
    "                                       remainder='passthrough')\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b107941",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the col transformer to learn feature names \n",
    "preprocessor.fit(X_train)\n",
    "## Now create list of our final features after preprocessing\n",
    "final_features = []\n",
    "## adding the numeric features which process first in the Col Trans\n",
    "final_features.extend(num_cols)\n",
    "## Now, get OHe feature names\n",
    "cat_features = preprocessor.named_transformers_['pipeline-2'][-1].get_feature_names_out(cat_cols)\n",
    "final_features.extend(cat_features)\n",
    "## Transform X vars and remake as dataframes\n",
    "X_train_df = pd.DataFrame( preprocessor.transform(X_train), \n",
    "                         columns=final_features, \n",
    "                         index=X_train.index)\n",
    "X_test_df = pd.DataFrame( preprocessor.transform(X_test), \n",
    "                         columns=final_features, \n",
    "                         index=X_test.index)\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding constants for statsmodels\n",
    "X_train_df = sm.add_constant(X_train_df, prepend=False)\n",
    "X_test_df = sm.add_constant(X_test_df, prepend=False)\n",
    "X_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef809ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make & fit a statmsodels OLS\n",
    "model = sm.OLS(y_train,X_train_df)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0993607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get our residuals from statsmodels and preview first 5\n",
    "resid = result.resid\n",
    "resid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f17ad",
   "metadata": {},
   "source": [
    "# **Checking Assumption of Homoscedasticity and Checking the Assumption of Normality with Q-QPlots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ols(result,X_train_df, y_train):\n",
    "    \"\"\"Plots a Q-Q Plot and residual plot for a statsmodels OLS regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## save residuals from result\n",
    "    y_pred = result.predict(X_train_df)\n",
    "    resid = y_train - y_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2,figsize=(12,5))\n",
    "    ## Normality \n",
    "    sm.graphics.qqplot(resid,line='45',fit=True,ax=axes[0]);\n",
    "    \n",
    "    ## Homoscedasticity\n",
    "    ax = axes[1]\n",
    "    ax.scatter(y_pred, resid, edgecolor='white',lw=1)\n",
    "    ax.axhline(0,zorder=0)\n",
    "    ax.set(ylabel='Residuals',xlabel='Predicted Value');\n",
    "    plt.tight_layout()\n",
    "    \n",
    "evaluate_ols(result,X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "z_price = scaler.fit_transform(y_train.values.reshape(-1,1))\n",
    "z_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be373768",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_price = pd.Series(z_price.flatten(),index=y_train.index )\n",
    "z_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the true/false result as our outlier index\n",
    "idx_outliers= z_price>3\n",
    "idx_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407af297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many outliers did we find?\n",
    "idx_outliers.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a cleaned version of y_train and X_train with outliers removed\n",
    "y_train_cln = y_train[~idx_outliers]\n",
    "X_train_cln = X_train_df[~idx_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39547678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our model includes Revenue:\")\n",
    "print(f\"- Greater than ${y_train_cln.min():,.2f}\")\n",
    "print(f\"- Less than ${y_train_cln.max():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting scaled y_test\n",
    "z_price_test = scaler.transform(y_test.values.reshape(-1,1))\n",
    "z_price_test = pd.Series(z_price_test.flatten(),index=y_test.index )\n",
    "# saving the true/false result as our outlier index\n",
    "idx_outliers_test= z_price_test>3\n",
    "# how many outleirs in test data?\n",
    "idx_outliers_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c04744",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make clean version of X_test and y_test\n",
    "X_test_cln = X_test_df[~idx_outliers_test] \n",
    "y_test_cln = y_test[~idx_outliers_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make & fit a statmsodels OLS\n",
    "model = sm.OLS(y_train_cln,X_train_cln)\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "evaluate_ols(result,X_train_cln,y_train_cln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c38471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save p-values\n",
    "p_vals = result.pvalues\n",
    "## filter for p_values that are >.05\n",
    "p_vals[p_vals>.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so how many Series columns do we have overall? \n",
    "# use a list comprehension to filter out column sthat start with zipcode\n",
    "Series_cols = [col for col in X_train_df.columns if col.startswith('Series')]\n",
    "# preview first few zipcode cols to confirm\n",
    "Series_cols[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Series_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So how many zipcode coeffs are insig?\n",
    "len(p_vals[p_vals>.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate test\n",
    "r2_test = r2_score(y_test_cln, result.predict(X_test_cln))\n",
    "print(f\"R-Squared for Test Data: {r2_test:.2f}\")\n",
    "evaluate_ols(result,X_test_cln, y_test_cln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ols(result,X_train_df, y_train):\n",
    "    \"\"\"Plots a Q-Q Plot and residual plot for a statsmodels OLS regression.\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Make predictions and calculate residuals\n",
    "    y_pred = result.predict(X_train_df)\n",
    "    resid = y_train - y_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2,figsize=(12,5))\n",
    "    ## Normality \n",
    "    sm.graphics.qqplot(resid, line='45',fit=True,ax=axes[0]);\n",
    "    \n",
    "    ## Homoscedascity\n",
    "    ax = axes[1]\n",
    "    ax.scatter(y_pred, resid, edgecolor='white',lw=1)\n",
    "    ax.axhline(0,zorder=0)\n",
    "    ax.set(ylabel='Residuals',xlabel='Predicted Value');\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae4579",
   "metadata": {},
   "source": [
    "# **Saving Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bed763",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a dictionary of all of the variables to save for later\n",
    "export = {'X_train':X_train_cln,\n",
    "         'y_train':y_train_cln,\n",
    "         'X_test':X_test_cln,\n",
    "         'y_test':y_test_cln,\n",
    "          'Outlier Scaler':scaler,\n",
    "          'Column Transformer':preprocessor,\n",
    "         'OLS Results': result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a499be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gzip file containing the joblib file\n",
    "joblib.dump(export, 'ols_results.joblib.gz', compress=('gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015993aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = joblib.load('ols_results.joblib.gz')\n",
    "loaded_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the dictionary data into separate variables\n",
    "X_train_df = loaded_data['X_train']\n",
    "y_train = loaded_data['y_train']\n",
    "X_test_df  = loaded_data['X_test']\n",
    "y_test = loaded_data['y_test']\n",
    "##  Saving the model and processing tools to new vars\n",
    "result = loaded_data['OLS Results']\n",
    "outlier_scaler = loaded_data['Outlier Scaler']\n",
    "preprocessor = loaded_data['Column Transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ee5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.summary())\n",
    "evaluate_ols(result,X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c84f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting statsmodels coefficients\n",
    "coeffs = result.params\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61211de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the True/False for which are zipcode cols\n",
    "genres_cols = coeffs.index.str.contains('genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## slicing out the zicode coefficents to a separate series\n",
    "coeffs_genres = coeffs.loc[genres_cols].copy()\n",
    "coeffs_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b77ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing out the zicode coefficents to a separate series\n",
    "# coeffs_main will have all coefficients EXCEPT those for zip code\n",
    "coeffs_main = coeffs.loc[~genres_cols].copy()\n",
    "coeffs_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting the coefficients largest to smallest\n",
    "coeffs_main = coeffs_main.sort_values(ascending=False)\n",
    "coeffs_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## visualizing variables with large negative ceoffs\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12,5),\n",
    "                         sharey=True)\n",
    "sns.regplot(x=X_train_df['averageRating'], y=y_train,\n",
    "            scatter_kws={'ec':'white','lw':1},\n",
    "            line_kws={'color':'red'}, ax=axes[0])\n",
    "axes[0].set_title('Average Rating vs Revenue')\n",
    "sns.regplot(x=X_train_df['runtimeMinutes'], y=y_train,\n",
    "            scatter_kws={'ec':'white','lw':1},\n",
    "            line_kws={'color':'red'}, ax=axes[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = coeffs_genres.sort_values().plot(kind='bar',figsize=(12,8))\n",
    "ax.axhline()\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('${x:,.2f}'))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b157898",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_genres = X_train_df.loc[:,~genres_cols]\n",
    "X_test_no_genres = X_test_df.loc[:,~genres_cols]\n",
    "display(X_train_no_genres.head(2),\n",
    "       X_test_no_genres.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting a model without zipcodes \n",
    "model = sm.OLS(y_train,X_train_no_genres)\n",
    "result = model.fit()\n",
    "display(result.summary())\n",
    "evaluate_ols(result,X_train_no_genres, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.352px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
